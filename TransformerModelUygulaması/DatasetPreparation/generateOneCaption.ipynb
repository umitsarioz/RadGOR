{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from pickle import load\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "\n",
    "class PositionEncoder:\n",
    "    def aci_hesapla(self, pos, i, embed_dim):\n",
    "        aci_orani = 1 / np.power(10000, (2 * (i // 2)) / np.float32(embed_dim))\n",
    "        return pos * aci_orani\n",
    "\n",
    "    def toDecoder(self, pos, embed_dim):\n",
    "        aci_orani = self.aci_hesapla(np.arange(pos)[:, np.newaxis], np.arange(embed_dim)[np.newaxis, :], embed_dim)\n",
    "\n",
    "        aci_orani[:, 0::2] = np.sin(aci_orani[:, 0::2])\n",
    "        aci_orani[:, 1::2] = np.cos(aci_orani[:, 1::2])\n",
    "        pos_encoding = aci_orani[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def toEncoder(self, satir, sutun, embed_dim):\n",
    "        assert embed_dim % 2 == 0\n",
    "        satir_pos = np.repeat(np.arange(satir), sutun)[:, np.newaxis]\n",
    "        sutun_pos = np.repeat(np.expand_dims(np.arange(sutun), 0), satir, axis=0).reshape(-1, 1)\n",
    "\n",
    "        aci_orani_satir = self.aci_hesapla(satir_pos, np.arange(embed_dim // 2)[np.newaxis, :], embed_dim // 2)\n",
    "        aci_orani_sutun = self.aci_hesapla(sutun_pos, np.arange(embed_dim // 2)[np.newaxis, :], embed_dim // 2)\n",
    "\n",
    "        aci_orani_satir[:, 0::2] = np.sin(aci_orani_satir[:, 0::2])\n",
    "        aci_orani_satir[:, 1::2] = np.cos(aci_orani_satir[:, 1::2])\n",
    "        aci_orani_sutun[:, 0::2] = np.sin(aci_orani_sutun[:, 0::2])\n",
    "        aci_orani_sutun[:, 1::2] = np.cos(aci_orani_sutun[:, 1::2])\n",
    "        pos_encoding = np.concatenate([aci_orani_satir, aci_orani_sutun], axis=1)[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, nheads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.nheads = nheads\n",
    "        self.embed_dim = embed_dim\n",
    "        assert embed_dim % self.nheads == 0\n",
    "        self.depth = embed_dim // self.nheads\n",
    "        self.wq = tf.keras.layers.Dense(embed_dim)\n",
    "        self.wk = tf.keras.layers.Dense(embed_dim)\n",
    "        self.wv = tf.keras.layers.Dense(embed_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embed_dim)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.nheads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)  # (batch_size, seq_len, embed_dim)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, embed_dim)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, nheads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, nheads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, nheads, seq_len_v, depth)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, nheads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.embed_dim))  # (batch_size, seq_len_q, embed_dim)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, embed_dim)\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(embed_dim, ffn_dim):\n",
    "    return tf.keras.Sequential([tf.keras.layers.Dense(ffn_dim, activation='relu'),\n",
    "                                tf.keras.layers.Dense(embed_dim)])  # (batch_size, seq_len, embed_dim)])\n",
    "\n",
    "\n",
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(embed_dim, ffn_dim)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask=None):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, embed_dim)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, embed_dim)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, embed_dim)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, embed_dim)\n",
    "        return out2\n",
    "\n",
    "\n",
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, embed_dim, nheads, ffn_dim, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mha1 = MultiHeadAttention(embed_dim, nheads)\n",
    "        self.mha2 = MultiHeadAttention(embed_dim, nheads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(embed_dim, ffn_dim)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, embed_dim)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, embed_dim)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, embed_dim)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, embed_dim)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, num_layers, embed_dim, nheads, ffn_dim, row_size, col_size, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Dense(self.embed_dim, activation='relu')\n",
    "        self.pos_encoding = PositionEncoder().toEncoder(row_size, col_size, self.embed_dim)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(embed_dim, nheads, ffn_dim, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len(H*W), embed_dim)\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, embed_dim)\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, num_layers, embed_dim, nheads, ffn_dim, vocabsize, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocabsize, embed_dim)\n",
    "        self.pos_encoding = PositionEncoder().toDecoder(vocabsize, embed_dim)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(embed_dim, nheads, ffn_dim, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask=None, padding_mask=None):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, embed_dim)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n",
    "\n",
    "        return x, attention_weights\n",
    "\n",
    "\n",
    "class Transformer(Model):\n",
    "    def __init__(self, num_layers, embed_dim, nheads, ffn_dim, row_size, col_size, vocabsize, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(num_layers, embed_dim, nheads, ffn_dim, row_size, col_size, rate)\n",
    "        self.decoder = Decoder(num_layers, embed_dim, nheads, ffn_dim, vocabsize, rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(vocabsize)\n",
    "\n",
    "    def call(self, inp, tar, training, look_ahead_mask=None, dec_padding_mask=None, enc_padding_mask=None):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, embed_dim  )\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, vocabsize)\n",
    "        return final_output\n",
    "\n",
    "\n",
    "def loadTokenizer(filepath):\n",
    "    tknzr,maxlen = list(load(open(filepath,'rb')).values())\n",
    "    return tknzr,maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goruntuYukle(image_path):\n",
    "   img = tf.io.read_file(image_path)\n",
    "   img = tf.image.decode_png(img, channels=3)\n",
    "   img = tf.image.resize(img, (299, 299))\n",
    "   img = preprocess_input(img)\n",
    "   return img\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "   seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "   return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "   mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "   return mask  # (seq_len, seq_len)\n",
    "\n",
    "def create_masks_decoder(tar):\n",
    "   look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "   dec_target_padding_mask = create_padding_mask(tar)\n",
    "   combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "   return combined_mask\n",
    "\n",
    "\n",
    "def generateOneCaption(image_path,extraction_model,prediction_model,tokenizer,SEQ_LENGTH):\n",
    "   temp_input = tf.expand_dims(goruntuYukle(image_path), 0)\n",
    "   img_tensor_val = extraction_model(temp_input)\n",
    "   img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "   start_token = tokenizer.word_index['<start>']\n",
    "   end_token = tokenizer.word_index['<end>']\n",
    "   decoder_input = [start_token]\n",
    "\n",
    "   output = tf.expand_dims(decoder_input, 0) #tokens\n",
    "   predicted_words = [] #word list\n",
    "   #print(\"Inıtial: Outpu:\",output,\"\\nDeocder_inp:\",decoder_input,\"\\nstart_token:\",start_token)\n",
    "   for i in range(SEQ_LENGTH):\n",
    "      dec_mask = create_masks_decoder(output)\n",
    "      predictions = prediction_model(img_tensor_val,output,False,dec_mask)\n",
    "      predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "      predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "      if predicted_id == end_token:\n",
    "         predicted_caption = [' '.join(pred_word for pred_word in predicted_words)]\n",
    "         return predicted_caption[0] # tek elemanlı liste\n",
    "      predicted_words.append(tokenizer.index_word[int(predicted_id)])\n",
    "      output = tf.concat([output, predicted_id], axis=-1)\n",
    "      #print(\"output:\",output,\"\\noutput.shape:\",output.shape)\n",
    "      #print(\"predicted_id:\",predicted_id)\n",
    "   predicted_caption = [' '.join(pred_word for pred_word in result)]\n",
    "   return predicted_caption[0] # tek elemanlı liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('TümBM496_Dosyalari/bm496_7haziran')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umtsr\\Desktop\\TümBM496_Dosyalari\\bm496_7haziran\\arayüz_modülü\n"
     ]
    }
   ],
   "source": [
    "CURR_DIR = os.path.join(os.getcwd(),'arayüz_modülü')\n",
    "print(CURR_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ornek_images_fp = os.path.join(os.getcwd(),'dataset','frontal_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ornek_imagesfp: C:\\Users\\umtsr\\Desktop\\TümBM496_Dosyalari\\bm496_7haziran\\dataset\\frontal_images\\1002_IM-0004-1001.png\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model_weights_path = os.path.join(CURR_DIR, 'modeller', 'mytransformer_model_weights','mytransformer_model_weights')\n",
    "extract_model_path = os.path.join(CURR_DIR, 'modeller', 'extraction_model')\n",
    "tokenizer_filepath =  os.path.join(CURR_DIR, 'modeller', 'mytknzr.pkl')\n",
    "\n",
    "tknzr,maxlen = loadTokenizer(tokenizer_filepath)\n",
    "VOCABSIZE = len(tknzr.index_word)+1\n",
    "NUM_LAYER = 6 # encoder-decoder blok tekrar sayısı\n",
    "EMBED_DIM = 512 # embedding dimension\n",
    "FFN_DIM = 3072 #feed-forward network dimension\n",
    "NHEADS = 8 # kafa sayısı\n",
    "ROW_SIZE = 8 # encoder-input- hizalama satir sayisi\n",
    "COL_SIZE = 8 # encoder-input- hizalama sutun sayisi\n",
    "DROPOUT_RATE = 0.1 # dropout oranı\n",
    "\n",
    "baseTransformerModel = Transformer(NUM_LAYER, EMBED_DIM, NHEADS, FFN_DIM,ROW_SIZE,COL_SIZE, VOCABSIZE, rate=DROPOUT_RATE)\n",
    "baseTransformerModel.load_weights(model_weights_path)\n",
    "\n",
    "extract_model = load_model(extract_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ornek_imagesfp: C:\\Users\\umtsr\\Desktop\\TümBM496_Dosyalari\\bm496_7haziran\\dataset\\frontal_images\\1050_IM-0038-4004.png\n"
     ]
    }
   ],
   "source": [
    "ornek_image_filepath = os.path.join(ornek_images_fp,list(os.listdir(ornek_images_fp))[55])\n",
    "print(\"ornek_imagesfp:\",ornek_image_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tahmin: frontal lateral views chest show unchanged cardiomediastinal silhouette reduced lung volumes basilar atelectasis no focal airspace consolidation pleural effusion\n"
     ]
    }
   ],
   "source": [
    "tahmin = generateOneCaption(ornek_image_filepath,extract_model,baseTransformerModel,tknzr,maxlen)\n",
    "print(\"tahmin:\",tahmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
